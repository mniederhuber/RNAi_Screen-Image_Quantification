import numpy as np
import pandas as pd
from aicsimageio import AICSImage

from skimage import filters, io, morphology, feature, img_as_ubyte
from skimage.measure import label, regionprops
from tifffile import imsave, imread



#sample_sheet.tsv is generated in the current project directory by 'sheet_gen.py' and has correct file paths for .lif files
sample_sheet_LL = pd.read_csv('sheets/sample_sheet.tsv', sep = '\t')

## To Do: clean this up, results_fp.csv is a dependency that will break image_parser if lost. Need to internally generate a 
## BL stock# to gene symbol sheet -- or just switch to using 'results_ForR.csv'

#results_fp.csv is currently generated by an Rnotebook that modifies 'results _ForR.csv' 
sample_sheet = pd.read_csv('sheets/results_fp.csv')

sample_sheet['date'] = sample_sheet['date'].fillna(0).astype(int)
sample_sheet['date'] = sample_sheet['date'].astype(int)

symbol_dict = {}

#loop sample_sheet and get 'Symbol' 'line' and 'date' info for each row.
#then assign Bloomington lines to Gene Symbols in the symbol_dict dictionary
#this is necessary because sample_sheet_LL does not have gene symbols associated with line #s
sample_id = []
for x, y in sample_sheet.iterrows():
    symbol = y['Symbol']
    line = y['line']
    date = str(y['date'])
   
  #join the row info and append to the sample_id list 
    sample_id.append('_'.join([symbol,line,date]))
  #make a key of the Bloomington Line # of the current sample_sheet row with the gene Symbol as value
    symbol_dict[line] = symbol

#add the list of sample_ids to a new column in sample_sheet called 'id'    
sample_sheet['id'] = sample_id  
#match gene symbol to BL stock # and make a 'Symbol' column in sample_sheet_LL
sample_sheet_LL['Symbol'] = sample_sheet_LL['line'].map(symbol_dict)


#add a sample_id variable to sample_sheet_LL
sample_id = []
for x, y in sample_sheet_LL.iterrows():
    symbol = y['Symbol']
    line = y['line']
    date = str(y['date'])
    
    sample_id.append('_'.join([symbol,line,date]))
    
sample_sheet_LL['id'] = sample_id  

#outdata dictionary
outdata = {
    'symbol' : [],
    'line' : [],
    'date' : [],
    'scene' : [],
    'shape' : [],
    'size' : [],
    'stack' : [],
    'gfp':[],
    'brDisc':[],
    'brDisc_gfp' : [],
    'brDisc_gfpNEG' : [],
    'KDtoWT' : []
}

## Image Processing ##

n = 0
for sample_id in sample_sheet_LL['id']:
    percent_complete = round(100*(n/len(sample_sheet_LL.id)), 2)
    print(f"{percent_complete}% complete")

    #get image id variables
    line = sample_sheet_LL.loc[sample_sheet_LL['id'] == sample_id, 'line'].iloc[0]
    date = sample_sheet_LL.loc[sample_sheet_LL['id'] == sample_id, 'date'].iloc[0]
    fp = sample_sheet_LL.loc[sample_sheet_LL['id'] == sample_id, 'fp'].iloc[0]
    symbol = sample_sheet_LL.loc[sample_sheet_LL['id'] == sample_id, 'Symbol'].iloc[0]
   
    #load image 
    im = AICSImage(fp)
   
    for scene in im.scenes:

    #AICSImage will pull the data of the current scene only -- need to specify scene  
        im.set_scene(scene)

    #get scene values
        shape = im.shape
        array = im.data
        sample = sample_id
        stack = im.shape[2]        
        size = f"{im.shape[3]}x{im.shape[4]}"

    #check that scene has all 3 channels (DAPI, GFP, tdTomato)
        if im.shape[1] == 3:

    #make Zmax projections of each channel
            dapi = np.amax(im.data[:,0,:,:,:], axis = 1)
            gfp = np.amax(im.data[:,1,:,:,:], axis = 1)
            brDisc = np.amax(im.data[:,2,:,:,:], axis = 1)
            
            dapi_gauss = filters.gaussian(dapi, sigma = 5)
    #make a boolean mask of DAPI -- 0.1 was empirically chosen by testing in FIJI
            dapi_mask = dapi_gauss > 0.1

            gfp_gauss = filters.gaussian(gfp, sigma = 5)
    #cut out parts of GFP image where there is no DAPI
            gfp_cut = np.where(dapi_mask == False, 0, gfp_gauss)

    #make GFP mask, see above re: 0.1 cutoff
            gfp_mask = gfp_cut >= 0.1 
            #gfp_mask = gfp_gauss >= filters.threshold_otsu(gfp_gauss)             

    #make an inverse mask to GFP mask -- this serves as a mask for the WT part of the wing 
            gfpNEG_mask = np.array(np.subtract(gfp_mask, dapi_mask, dtype = float), dtype = bool)

    #get mean values of brDisc signal in the GFP masked region
            brDisc_gfp = np.zeros_like(brDisc)
            brDisc_gfp[gfp_mask] = brDisc[gfp_mask]
            brDisc_gfp_signal = np.mean(brDisc_gfp)

    #get mean values of brDisc signal in the GFP-negative masked region
            brDisc_gfpNEG = np.zeros_like(brDisc)
            brDisc_gfpNEG[gfpNEG_mask] = brDisc[gfpNEG_mask]
            brDisc_gfpNEG_signal = np.mean(brDisc_gfpNEG)

    #calculate the knockdown to WT ratio
            KDtoWT = brDisc_gfp_signal / brDisc_gfpNEG_signal
            
    #output max projections and masks to check accuracy
            io.imsave(f"output/{sample}_{scene}_GFP_MAX.tiff", gfp)
            io.imsave(f"output/{sample}_{scene}_GFP-MASK_MAX.tiff", img_as_ubyte(gfp_mask))
            io.imsave(f"output/{sample}_{scene}_GFPNEG-MASK_MAX.tiff", img_as_ubyte(gfpNEG_mask))
            io.imsave(f"output/{sample}_{scene}_DAPI_MAX.tiff", dapi)
            io.imsave(f"output/{sample}_{scene}_DAPI-MASK_MAX.tiff", img_as_ubyte(dapi_mask))
            io.imsave(f"output/{sample}_{scene}_brDisc_MAX.tiff", brDisc)
            
    #output values to the outdata dictionary
            outdata['symbol'].append(symbol)
            outdata['line'].append(line)
            outdata['date'].append(date)
            outdata['scene'].append(scene)
            outdata['shape'].append(shape)
            outdata['size'].append(size)
            outdata['stack'].append(stack)
            outdata['gfp'].append(np.mean(gfp))
            outdata['brDisc'].append(np.mean(brDisc))
            outdata['brDisc_gfp'].append(brDisc_gfp_signal)
            outdata['brDisc_gfpNEG'].append(brDisc_gfpNEG_signal)
            outdata['KDtoWT'].append(KDtoWT)

    n = n + 1

#build the outdata dataframe and save
df = pd.DataFrame(outdata)
pd.DataFrame.to_csv(df, 'output/outdata.csv')





